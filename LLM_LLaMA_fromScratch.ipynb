{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.12",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "none",
      "dataSources": [],
      "dockerImageVersionId": 30626,
      "isInternetEnabled": false,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": false
    },
    "colab": {
      "name": "notebook3eedc54454",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/azzouzioussama/LLM/blob/main/LLM_LLaMA_fromScratch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Overview\n",
        "\n",
        "In the [Customize building LLM in PyTorch](https://www.kaggle.com/code/aisuko/customize-building-llm-in-pytorch), we have been covered essential concepts. Here we will replicate our basic model to LLaMA architecture.\n",
        "\n",
        "**Note: The pictures all from the internet or the credit section below**\n",
        "\n",
        "LLaMA introduces three architectural modifications to the original Transformer:\n",
        "- RMSNorm for pre-normalization\n",
        "- Rotary embeedings\n",
        "- SwiGLU activation function\n",
        "\n",
        "\n",
        "<div style=\"text-align: center\"><img src=\"https://files.mastodon.social/media_attachments/files/111/615/341/791/154/468/small/847f74b69b31f5a1.webp\" width=\"60%\" heigh=\"80%\" alt=\"encoder and decoder in transformers\"></div>"
      ],
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "id": "beBw_8OiK9gu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "from torch import nn\n",
        "from torch.nn import functional as F\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "from matplotlib import pyplot as plt\n",
        "\n",
        "import time\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "import urllib.request"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-12-24T04:23:55.679634Z",
          "iopub.execute_input": "2023-12-24T04:23:55.68017Z",
          "iopub.status.idle": "2023-12-24T04:23:55.68695Z",
          "shell.execute_reply.started": "2023-12-24T04:23:55.680125Z",
          "shell.execute_reply": "2023-12-24T04:23:55.685992Z"
        },
        "trusted": true,
        "id": "bZVoRKmnK9gz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# The basic model"
      ],
      "metadata": {
        "id": "5Q_jGvAlK9g2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "url=\"https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\"\n",
        "\n",
        "file_name=\"tinyshakespeare.txt\"\n",
        "\n",
        "urllib.request.urlretrieve(url, file_name)\n",
        "\n",
        "lines=open(file_name, 'r').read()\n",
        "\n",
        "vocab=sorted(list(set(lines)))"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-12-24T04:23:55.688876Z",
          "iopub.execute_input": "2023-12-24T04:23:55.689328Z",
          "iopub.status.idle": "2023-12-24T04:23:56.303391Z",
          "shell.execute_reply.started": "2023-12-24T04:23:55.689292Z",
          "shell.execute_reply": "2023-12-24T04:23:56.301868Z"
        },
        "trusted": true,
        "id": "xAuJESh3K9g3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "DEFAULT_CONFIG={\n",
        "    'd_model':128,\n",
        "    'vocab_size':len(vocab),\n",
        "    'batch_size':8,\n",
        "    'context_window':16\n",
        "}"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-12-24T04:23:56.305592Z",
          "iopub.execute_input": "2023-12-24T04:23:56.306594Z",
          "iopub.status.idle": "2023-12-24T04:23:56.312251Z",
          "shell.execute_reply.started": "2023-12-24T04:23:56.306554Z",
          "shell.execute_reply": "2023-12-24T04:23:56.310883Z"
        },
        "trusted": true,
        "id": "0eJAf61vK9g3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "itos={i: ch for i, ch in enumerate(vocab)}\n",
        "\n",
        "stoi={ch: i for i, ch in enumerate(vocab)}\n",
        "\n",
        "def encode(s):\n",
        "    return [stoi[ch] for ch in s]\n",
        "\n",
        "def decode(l):\n",
        "    return ''.join(itos[i] for i in l)\n",
        "\n",
        "decode(encode(\"morning\"))"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-12-24T04:23:56.313768Z",
          "iopub.execute_input": "2023-12-24T04:23:56.314252Z",
          "iopub.status.idle": "2023-12-24T04:23:56.331522Z",
          "shell.execute_reply.started": "2023-12-24T04:23:56.314206Z",
          "shell.execute_reply": "2023-12-24T04:23:56.32996Z"
        },
        "trusted": true,
        "id": "e4Er2w3AK9g4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset=torch.tensor(encode(lines), dtype=torch.int8)\n",
        "dataset.shape"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-12-24T04:23:56.334826Z",
          "iopub.execute_input": "2023-12-24T04:23:56.335376Z",
          "iopub.status.idle": "2023-12-24T04:23:56.624261Z",
          "shell.execute_reply.started": "2023-12-24T04:23:56.335327Z",
          "shell.execute_reply": "2023-12-24T04:23:56.62306Z"
        },
        "trusted": true,
        "id": "ESXQoiVgK9g4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_batches(data, split, batch_size, context_window, config=DEFAULT_CONFIG):\n",
        "    # Split the dataset into training, validation, and test sets\n",
        "    train=data[:int(.8*len(data))]\n",
        "    val=data[int(.8 * len(data)): int(.9*len(data))]\n",
        "    test=data[int(.9 *len(data)):]\n",
        "\n",
        "    # Determine whcih split to use\n",
        "    batch_data=train\n",
        "    if split=='val':\n",
        "        batch_data=val\n",
        "    if split=='test':\n",
        "        batch_data=test\n",
        "\n",
        "    # Pick random starting points within the data\n",
        "    ix=torch.randint(0,batch_data.size(0)-context_window-1, (batch_size,))\n",
        "\n",
        "    # create input sequences (x) and corrsponding target sequences (y)\n",
        "    x=torch.stack([batch_data[i:i+context_window] for i in ix]).long()\n",
        "    y=torch.stack([batch_data[i+1:i+context_window+1] for i in ix]).long()\n",
        "\n",
        "    return x,y"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-12-24T04:23:56.625619Z",
          "iopub.execute_input": "2023-12-24T04:23:56.626791Z",
          "iopub.status.idle": "2023-12-24T04:23:56.639155Z",
          "shell.execute_reply.started": "2023-12-24T04:23:56.62674Z",
          "shell.execute_reply": "2023-12-24T04:23:56.637844Z"
        },
        "trusted": true,
        "id": "Pb6DfKRUK9g4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "xs, ys=get_batches(dataset, 'train', DEFAULT_CONFIG['batch_size'], DEFAULT_CONFIG['context_window'])\n",
        "\n",
        "decoded_samples=[(decode(xs[i].tolist()), decode(ys[i].tolist())) for i in range(len(xs))]\n",
        "\n",
        "decoded_samples"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-12-24T04:23:56.641098Z",
          "iopub.execute_input": "2023-12-24T04:23:56.641581Z",
          "iopub.status.idle": "2023-12-24T04:23:56.658608Z",
          "shell.execute_reply.started": "2023-12-24T04:23:56.641535Z",
          "shell.execute_reply": "2023-12-24T04:23:56.657428Z"
        },
        "trusted": true,
        "id": "s8F4f5kqK9g5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@torch.no_grad()\n",
        "def evaluate_loss(model, config=DEFAULT_CONFIG):\n",
        "    out={}\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    for split in [\"train\", \"val\"]:\n",
        "\n",
        "        losses=[]\n",
        "\n",
        "        for _ in range(10):\n",
        "            xb,yb=get_batches(dataset, split, config['batch_size'], config['context_window'])\n",
        "            _,loss=model(xb,yb)\n",
        "            losses.append(loss.item())\n",
        "\n",
        "        out[split]=np.mean(losses)\n",
        "    model.train()\n",
        "\n",
        "    return out"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-12-24T04:23:56.660715Z",
          "iopub.execute_input": "2023-12-24T04:23:56.663273Z",
          "iopub.status.idle": "2023-12-24T04:23:56.672947Z",
          "shell.execute_reply.started": "2023-12-24T04:23:56.663199Z",
          "shell.execute_reply": "2023-12-24T04:23:56.671381Z"
        },
        "trusted": true,
        "id": "BN8DwhlCK9g5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# RMSNorm for pre-normalization\n",
        "\n",
        "We are defininf an RMSNorm class. During the initialization, it registers a scale parameter.\n",
        "\n",
        "In the forward pass, it calculates the **Frobenius norm** of the input tensor and then normalizes the tensor. Finally, the tensor is scaled by the registered scale parameter. This function is designed for use in LLaMA to replace the LayerNorm operation."
      ],
      "metadata": {
        "id": "wu6yEbz-K9g6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class RMSNorm(nn.Module):\n",
        "    def __init__(self, layer_shape, eps=1e-8, bias=False):\n",
        "        super(RMSNorm, self).__init__()\n",
        "\n",
        "        # Registering a learnable parameter 'scale' as a parameter o fthe module\n",
        "        self.register_parameter('scale', nn.Parameter(torch.ones(layer_shape)))\n",
        "\n",
        "    def forward(self,x):\n",
        "        \"\"\"\n",
        "        Assumes shape is (batch, seq_len, d_model)\n",
        "        \"\"\"\n",
        "\n",
        "        #calculating the Frobenius norm, RMS=1/sqrt(N)* Frobenius norm\n",
        "        ff_rms=torch.linalg.norm(x, dim=(1,2))*x[0].numel() ** -.5\n",
        "\n",
        "        # normalizing the input tensor 'x' with respect to RMS\n",
        "        raw=x/ff_rms.unsqueeze(-1).unsqueeze(-1)\n",
        "\n",
        "        # scaling the normalized tensor using the learnable parameter 'scale'\n",
        "        return self.scale[:x.shape[1],:].unsqueeze(0) * raw"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-12-24T04:23:56.674617Z",
          "iopub.execute_input": "2023-12-24T04:23:56.675106Z",
          "iopub.status.idle": "2023-12-24T04:23:56.687394Z",
          "shell.execute_reply.started": "2023-12-24T04:23:56.675034Z",
          "shell.execute_reply": "2023-12-24T04:23:56.685917Z"
        },
        "trusted": true,
        "id": "mov87WZ8K9g6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SimpleModel_RMS(nn.Module):\n",
        "    def __init__(self, config=DEFAULT_CONFIG):\n",
        "        super().__init__()\n",
        "        self.config=config\n",
        "\n",
        "        #Embedding layer to convert character indices to vectors\n",
        "        self.embedding=nn.Embedding(config['vocab_size'], config['d_model'])\n",
        "\n",
        "        #RMSNorm layer for pre-normalization\n",
        "        self.rms=RMSNorm(config['context_window'], config['d_model'])\n",
        "\n",
        "        #Linear layers for modeling relationships between features\n",
        "        self.linear=nn.Sequential(\n",
        "            nn.Linear(config['d_model'], config['d_model']),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(config['d_model'], config['vocab_size']),\n",
        "        )\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        # Embedding layer converts character indices to vectors\n",
        "        x =self.embedding(idx)\n",
        "\n",
        "        # Linear layers for modeling relationships between features\n",
        "        logits=self.linear(x)\n",
        "\n",
        "        if targets is not None:\n",
        "            loss=F.cross_entropy(logits.view(-1, self.config['vocab_size']), targets.view(-1))\n",
        "            return logits, loss\n",
        "        else:\n",
        "            return logits"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-12-24T04:23:56.689203Z",
          "iopub.execute_input": "2023-12-24T04:23:56.689641Z",
          "iopub.status.idle": "2023-12-24T04:23:56.705701Z",
          "shell.execute_reply.started": "2023-12-24T04:23:56.689607Z",
          "shell.execute_reply": "2023-12-24T04:23:56.70451Z"
        },
        "trusted": true,
        "id": "cAhd-xbpK9g7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's execute the modified NN model with RMSNorm and observe the update number of parameters in the model, along with the loss:"
      ],
      "metadata": {
        "id": "Hvpz6EeTK9g7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "DEFAULT_CONFIG.update({\n",
        "    'epochs': 1000,\n",
        "    'log_interval':10,\n",
        "    'batch_size':32,\n",
        "})\n",
        "\n",
        "model=SimpleModel_RMS(DEFAULT_CONFIG)\n",
        "\n",
        "xs, ys=get_batches(dataset, 'train', DEFAULT_CONFIG['batch_size'], DEFAULT_CONFIG['context_window'])\n",
        "\n",
        "logits, loss=model(xs, ys)\n",
        "\n",
        "optimizer=torch.optim.Adam(model.parameters())"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-12-24T04:23:56.710743Z",
          "iopub.execute_input": "2023-12-24T04:23:56.711299Z",
          "iopub.status.idle": "2023-12-24T04:23:56.728692Z",
          "shell.execute_reply.started": "2023-12-24T04:23:56.71126Z",
          "shell.execute_reply": "2023-12-24T04:23:56.727085Z"
        },
        "trusted": true,
        "id": "uF8hG66qK9g7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(model, optimizer, scheduler=None, config=DEFAULT_CONFIG, print_logs=False):\n",
        "    losses=[]\n",
        "\n",
        "    start_time=time.time()\n",
        "\n",
        "    for epoch in range(config['epochs']):\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        xs, ys=get_batches(dataset, 'train', config['batch_size'], config['context_window'])\n",
        "\n",
        "        logits, loss=model(xs, targets=ys)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if scheduler:\n",
        "            scheduler.step()\n",
        "\n",
        "        if epoch%config['log_interval']==0:\n",
        "            batch_time=time.time()-start_time\n",
        "            x=evaluate_loss(model)\n",
        "\n",
        "            losses+=[x]\n",
        "\n",
        "            if print_logs:\n",
        "                print(f\"Epoch {epoch} | val loss {x['val']:.3f}|Time {batch_time:.3f}|ETA in seconds{batch_time*(config['epochs']-epoch)/config['log_interval']:.3f}\")\n",
        "\n",
        "            start_time=time.time()\n",
        "\n",
        "            if scheduler:\n",
        "                print(\"lr: \", scheduler.get_lr())\n",
        "    print(\"Validation loss:\", losses[-1]['val'])\n",
        "\n",
        "    return pd.DataFrame(losses).plot()\n",
        "\n",
        "train(model, optimizer)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-12-24T04:23:56.730201Z",
          "iopub.execute_input": "2023-12-24T04:23:56.730589Z",
          "iopub.status.idle": "2023-12-24T04:24:04.86154Z",
          "shell.execute_reply.started": "2023-12-24T04:23:56.730555Z",
          "shell.execute_reply": "2023-12-24T04:24:04.860032Z"
        },
        "trusted": true,
        "id": "NLfdEwsmK9g8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As we can see that the validation loss experiences a small decrease.\n",
        "\n",
        "\n",
        "# Rotary Embeddings\n",
        "\n",
        "Next, we will implement rotary positional embeddings. In RoPE, we need to embedding the position of a token in a sequence by rotating the embedding, applying a different rotation at each position. Let's create a function that mimics the actual paper implement of RoPE:"
      ],
      "metadata": {
        "id": "4IXVm_duK9g8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_rotary_matrix(context_window, embedding_dim):\n",
        "    # Initialize a tensor for the rotary matrix with zeros\n",
        "    R=torch.zeros((context_window, embedding_dim, embedding_dim), requires_grad=False)\n",
        "\n",
        "    # Loop thorugh each position in the context window\n",
        "    for position in range(context_window):\n",
        "        # Loop through each dimension in the embedding\n",
        "        for i in range(embedding_dim//2):\n",
        "            # Calculate the rotation angle (theta) based on the position and embedding dimension\n",
        "            theta=10000. ** (-2.*(i-1)/embedding_dim)\n",
        "            # Calculate the rotated matrix elements using sine and cosine functions\n",
        "            m_theta=position*theta\n",
        "            R[position, 2*i, 2*i]=np.cos(m_theta)\n",
        "            R[position, 2*i, 2*i+1]=-np.sin(m_theta)\n",
        "            R[position, 2*i+1, 2*i]=np.sin(m_theta)\n",
        "            R[position, 2*i+1, 2*i+1]=np.cos(m_theta)\n",
        "    return R"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-12-24T04:24:04.863389Z",
          "iopub.execute_input": "2023-12-24T04:24:04.863917Z",
          "iopub.status.idle": "2023-12-24T04:24:04.874952Z",
          "shell.execute_reply.started": "2023-12-24T04:24:04.863863Z",
          "shell.execute_reply": "2023-12-24T04:24:04.873481Z"
        },
        "trusted": true,
        "id": "5vLfzxgnK9g8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We generate a rotary matrix based on the specified context window and embedding dimension, following the proposed RoPE implementation.\n",
        "\n",
        "\n",
        "## Masked Attention Head\n",
        "\n",
        "We know that the attention heads in the architecture of transformers.\n",
        "\n",
        "We need to create attention heads when replicating LLaMA. Let's first create a single **masked attention head** that returns attention weights."
      ],
      "metadata": {
        "id": "xt9oNDm1K9g8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class RoPEMaskedAttentionHead(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.config=config\n",
        "        # Linear transformation for query\n",
        "        self.w_q=nn.Linear(config['d_model'], config['d_model'], bias=False)\n",
        "        # Linear transformation for key\n",
        "        self.w_k=nn.Linear(config['d_model'], config['d_model'], bias=False)\n",
        "        # Linear transformation for value\n",
        "        self.w_v=nn.Linear(config['d_model'], config['d_model'], bias=False)\n",
        "        # Obtain rotary matrix for positional embeddings\n",
        "        self.R=get_rotary_matrix(config['context_window'], config['d_model'])\n",
        "\n",
        "    def get_rotary_matrix(context_window, embedding_dim):\n",
        "        # Initialize a tensor for the rotary matrix with zeros\n",
        "        R=torch.zeros((context_window, embedding_dim, embedding_dim), requires_grad=False)\n",
        "\n",
        "        # Loop thorugh each position in the context window\n",
        "        for position in range(context_window):\n",
        "            # Loop through each dimension in the embedding\n",
        "            for i in range(embedding_dim//2):\n",
        "                # Calculate the rotation angle (theta) based on the position and embedding dimension\n",
        "                theta=10000. ** (-2.*(i-1)/embedding_dim)\n",
        "                # Calculate the rotated matrix elements using sine and cosine functions\n",
        "                m_theta=position*theta\n",
        "                R[position, 2*i, 2*i]=np.cos(m_theta)\n",
        "                R[position, 2*i, 2*i+1]=-np.sin(m_theta)\n",
        "                R[position, 2*i+1, 2*i]=np.sin(m_theta)\n",
        "                R[position, 2*i+1, 2*i+1]=np.cos(m_theta)\n",
        "        return R\n",
        "\n",
        "    def forward(self, x, return_attn_weights=False):\n",
        "        # x: input tensor of shape (batch, sequence length, dimension)\n",
        "        b,m,d=x.shape # batch size, sequence length, dimension\n",
        "\n",
        "        # Linear transformations for Q, K and V\n",
        "        q=self.w_q(x)\n",
        "        k=self.w_k(x)\n",
        "        v=self.w_v(x)\n",
        "\n",
        "        # Rotate Q and K using the RoPE matrix\n",
        "        q_rotated=(torch.bmm(q.transpose(0,1), self.R[:m])).transpose(0,1)\n",
        "        k_rotated=(torch.bmm(k.transpose(0,1), self.R[:m])).transpose(0,1)\n",
        "\n",
        "        # Perform scaled dot-product attention\n",
        "        activations=F.scaled_dot_product_attention(\n",
        "            q_rotated, k_rotated, v, dropout_p=0.1, is_causal=True\n",
        "        )\n",
        "\n",
        "        if return_attn_weights:\n",
        "            # Create a causal attention mask\n",
        "            atten_mask=torch.tril(torch.ones((m,m)), diagnoal=0)\n",
        "            # Calculate attention weights and add causal mask\n",
        "            attn_weights =torch.bmm(q_rotated, k_rotated.transpose(1,2))/np.sqrt(d)+attn_mask\n",
        "            attn_weights=F.softmax(attn_weights, dim=-1)\n",
        "            return activations, attn_weights\n",
        "        return activations"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-12-24T04:24:04.877162Z",
          "iopub.execute_input": "2023-12-24T04:24:04.878083Z",
          "iopub.status.idle": "2023-12-24T04:24:04.898857Z",
          "shell.execute_reply.started": "2023-12-24T04:24:04.878015Z",
          "shell.execute_reply": "2023-12-24T04:24:04.897056Z"
        },
        "trusted": true,
        "id": "twenhrReK9g8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Multi-Head attention machanism"
      ],
      "metadata": {
        "id": "jk5RD31rK9g9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class RoPEMaskedMultiheadAttention(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.config=config\n",
        "        # Create a list of RoPEMaskedAttentionHead instances as attention heads\n",
        "        self.heads=nn.ModuleList([\n",
        "            RoPEMaskedAttentionHead(config) for _ in range(config['n_heads'])\n",
        "        ])\n",
        "        # Linear layer after concatenating heads\n",
        "        self.linear=nn.Linear(config['n_heads']* config['d_model'], config['d_model'])\n",
        "        self.dropout=nn.Dropout(.1) # Dropout layer\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x: input tensor of shape (batch, sequence length, dimension)\n",
        "\n",
        "        # Process each attention head and concatenate the results\n",
        "        heads=[h(x) for h in self.heads]\n",
        "        x=torch.cat(heads, dim=-1)\n",
        "\n",
        "        # Apply linear transformation to the concatenated output\n",
        "        x=self.linear(x)\n",
        "\n",
        "        # Apply dropout\n",
        "        x=self.dropout(x)\n",
        "        return x"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-12-24T04:24:04.900541Z",
          "iopub.execute_input": "2023-12-24T04:24:04.901029Z",
          "iopub.status.idle": "2023-12-24T04:24:04.915812Z",
          "shell.execute_reply.started": "2023-12-24T04:24:04.900977Z",
          "shell.execute_reply": "2023-12-24T04:24:04.914608Z"
        },
        "trusted": true,
        "id": "ps5RRieMK9hC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The original paper used 32 heads for their smaller 7b LLM variation, but due to constraints, we will use 8 heads for our approach"
      ],
      "metadata": {
        "id": "SJpfngAlK9hC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "DEFAULT_CONFIG.update({\n",
        "    'n_heads':8\n",
        "})"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-12-24T04:24:04.917271Z",
          "iopub.execute_input": "2023-12-24T04:24:04.917625Z",
          "iopub.status.idle": "2023-12-24T04:24:04.931056Z",
          "shell.execute_reply.started": "2023-12-24T04:24:04.917594Z",
          "shell.execute_reply": "2023-12-24T04:24:04.930118Z"
        },
        "trusted": true,
        "id": "0qL3CN4MK9hC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's update the previously RMSNorm code by using:\n",
        "\n",
        "- Rotational Embedding\n",
        "- Multi-Head-attention"
      ],
      "metadata": {
        "id": "3Xq1uBevK9hC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class RopeModel(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.config=config\n",
        "\n",
        "        #embedding layer for input tokens\n",
        "        self.embedding =nn.Embedding(config['vocab_size'], config['d_model'])\n",
        "\n",
        "        #RMSNorm layer for pre-normalization ---------------------------------------- IT Must be TUPLE\n",
        "        self.rms=RMSNorm((config['context_window'], config['d_model']))\n",
        "\n",
        "        #RoPEMaskedMultiheadAttention layer\n",
        "        self.rope_attention=RoPEMaskedMultiheadAttention(config)\n",
        "\n",
        "        #Linear layer forward by ReLU activation\n",
        "        self.linear=nn.Sequential(\n",
        "            nn.Linear(config['d_model'], config['d_model']),\n",
        "            nn.ReLU(),\n",
        "        )\n",
        "\n",
        "\n",
        "        self.last_linear = nn.Linear(config['d_model'], config['vocab_size'])\n",
        "\n",
        "        #final linear layer for prediction\n",
        "        self.last_linear=nn.Linear(config['d_model'], config['vocab_size'])\n",
        "\n",
        "        print (\"Model params:\",sum([m.numel() for m in self.parameters()]))\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        #idx: input indices\n",
        "        x=self.embedding(idx)\n",
        "\n",
        "        #One block of attention\n",
        "        x=self.rms(x)\n",
        "        x=x+self.rope_attention(x)\n",
        "        #RMS pre-normalization\n",
        "        x=self.rms(x)\n",
        "        x=x+self.linear(x)\n",
        "\n",
        "        logits=self.last_linear(x)\n",
        "\n",
        "        if targets is not None:\n",
        "            loss=F.cross_entropy(logits.view(-1, self.config['vocab_size']), targets.view(-1))\n",
        "            return logits, loss\n",
        "        else:\n",
        "            return logits\n",
        ""
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-12-24T04:24:04.932581Z",
          "iopub.execute_input": "2023-12-24T04:24:04.933198Z",
          "iopub.status.idle": "2023-12-24T04:24:04.9483Z",
          "shell.execute_reply.started": "2023-12-24T04:24:04.93316Z",
          "shell.execute_reply": "2023-12-24T04:24:04.946223Z"
        },
        "trusted": true,
        "id": "uGGlWWYPK9hD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model=RopeModel(DEFAULT_CONFIG)\n",
        "\n",
        "xs, ys=get_batches(dataset, 'train', DEFAULT_CONFIG['batch_size'], DEFAULT_CONFIG['context_window'])\n",
        "\n",
        "logits, loss=model(xs, ys)\n",
        "\n",
        "optimizer=torch.optim.Adam(model.parameters())\n",
        "\n",
        "train(model, optimizer)"
      ],
      "metadata": {
        "trusted": true,
        "id": "mdRZP1f8K9hD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "DEFAULT_CONFIG.update({\n",
        "    \"epochs\":5000,\n",
        "    \"log_interval\":10,\n",
        "})\n",
        "\n",
        "train(model, optimizer)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-12-24T04:24:56.260995Z",
          "iopub.status.idle": "2023-12-24T04:24:56.261488Z",
          "shell.execute_reply.started": "2023-12-24T04:24:56.261262Z",
          "shell.execute_reply": "2023-12-24T04:24:56.261286Z"
        },
        "trusted": true,
        "id": "-VwA3IukK9hD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Credit\n",
        "\n",
        "https://levelup.gitconnected.com/building-a-million-parameter-llm-from-scratch-using-python-f612398f06c2#21f1"
      ],
      "metadata": {
        "id": "JJBsvOrCK9hD"
      }
    }
  ]
}