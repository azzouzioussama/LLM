{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/azzouzioussama/LLM/blob/main/6_2_Embeddings.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yUuby3g8ouzm"
      },
      "source": [
        "\n",
        "# üß† **Understanding Embeddings with BERT**\n",
        "\n",
        "In this notebook, we go into the world of embeddings, utilizing the BERT model to understand the semantic relationships between words in different contexts.\n",
        "\n",
        "## üõ†Ô∏è Setup and Installation\n",
        "\n",
        "Start by installing the necessary libraries to ensure all functionalities are available."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "6C1auKaP5I93",
        "outputId": "495921be-4b51-4182-af3b-295d6cd7683e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in /home/koukou/anaconda3/envs/dl_env/lib/python3.9/site-packages (4.41.2)\n",
            "Requirement already satisfied: filelock in /home/koukou/anaconda3/envs/dl_env/lib/python3.9/site-packages (from transformers) (3.13.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.0 in /home/koukou/anaconda3/envs/dl_env/lib/python3.9/site-packages (from transformers) (0.23.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /home/koukou/anaconda3/envs/dl_env/lib/python3.9/site-packages (from transformers) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /home/koukou/anaconda3/envs/dl_env/lib/python3.9/site-packages (from transformers) (24.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /home/koukou/anaconda3/envs/dl_env/lib/python3.9/site-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /home/koukou/anaconda3/envs/dl_env/lib/python3.9/site-packages (from transformers) (2024.5.15)\n",
            "Requirement already satisfied: requests in /home/koukou/anaconda3/envs/dl_env/lib/python3.9/site-packages (from transformers) (2.32.2)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /home/koukou/anaconda3/envs/dl_env/lib/python3.9/site-packages (from transformers) (0.19.1)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /home/koukou/anaconda3/envs/dl_env/lib/python3.9/site-packages (from transformers) (0.4.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /home/koukou/anaconda3/envs/dl_env/lib/python3.9/site-packages (from transformers) (4.66.4)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /home/koukou/anaconda3/envs/dl_env/lib/python3.9/site-packages (from huggingface-hub<1.0,>=0.23.0->transformers) (2024.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/koukou/anaconda3/envs/dl_env/lib/python3.9/site-packages (from huggingface-hub<1.0,>=0.23.0->transformers) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /home/koukou/anaconda3/envs/dl_env/lib/python3.9/site-packages (from requests->transformers) (2.0.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /home/koukou/anaconda3/envs/dl_env/lib/python3.9/site-packages (from requests->transformers) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/koukou/anaconda3/envs/dl_env/lib/python3.9/site-packages (from requests->transformers) (2.2.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /home/koukou/anaconda3/envs/dl_env/lib/python3.9/site-packages (from requests->transformers) (2024.6.2)\n",
            "Note: you may need to restart the kernel to use updated packages.\n",
            "Requirement already satisfied: scipy in /home/koukou/anaconda3/envs/dl_env/lib/python3.9/site-packages (1.13.1)\n",
            "Requirement already satisfied: numpy<2.3,>=1.22.4 in /home/koukou/anaconda3/envs/dl_env/lib/python3.9/site-packages (from scipy) (1.23.5)\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "%pip install transformers\n",
        "%pip install scipy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mpAjguA9o7tT"
      },
      "source": [
        "## üìö Importing Libraries\n",
        "\n",
        "Import essential modules for our tasks."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "DBwmRlXP5zY6"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/koukou/anaconda3/envs/dl_env/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ],
      "source": [
        "from transformers import BertModel, AutoTokenizer\n",
        "from scipy.spatial.distance import cosine"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XsfQ55vto_Bd"
      },
      "source": [
        "## ü§ñ Model Setup\n",
        "\n",
        "Load the pre-trained BERT model and tokenizer. This model will help us extract embeddings for our analysis."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L-JQ5YWeJB6Y",
        "outputId": "2339c7a7-9f04-490d-ac0a-d691668c4544"
      },
      "outputs": [],
      "source": [
        "# Defining the model name\n",
        "model_name = \"bert-base-cased\"\n",
        "\n",
        "# Loading the pre-trained model and tokenizer\n",
        "model = BertModel.from_pretrained(model_name)\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZzoQD4KrpClk"
      },
      "source": [
        "\n",
        "## üìù Function Definition: Predict\n",
        "\n",
        "Define a function that encodes input text into tensors, which are then fed to the model to obtain embeddings."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "GJIJ1VUZJBtp"
      },
      "outputs": [],
      "source": [
        "# Defining a function to encode the input text and get model predictions\n",
        "def predict(text):\n",
        "    encoded_inputs = tokenizer(text, return_tensors=\"pt\")\n",
        "    return model(**encoded_inputs)[0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eGyuVbYkpEfU"
      },
      "source": [
        "## üìÉ Defining the Sentences\n",
        "\n",
        "Set up sentences to analyze. The"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "s_0dKdRbJFtL"
      },
      "outputs": [],
      "source": [
        "# Defining the sentences\n",
        "sentence1 = \"There was a fly drinking from my soup\"\n",
        "sentence2 = \"There is a fly swimming in my juice\"\n",
        "# sentence2 = \"To become a commercial pilot, he had to fly for 1500 hours.\" # second fly example\n",
        "\n",
        "# Tokenizing the sentences\n",
        "tokens1 = tokenizer.tokenize(sentence1)\n",
        "tokens2 = tokenizer.tokenize(sentence2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fTn6NoNepG16"
      },
      "source": [
        "## üîç Tokenization and Model Predictions\n",
        "\n",
        "Tokenize the sentences and obtain predictions (embeddings) from the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "5B0LO9xfJI2P"
      },
      "outputs": [],
      "source": [
        "# Getting model predictions for the sentences\n",
        "out1 = predict(sentence1)\n",
        "out2 = predict(sentence2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mb8tPoc3pJxA"
      },
      "source": [
        "## üîÑ Extracting Embeddings\n",
        "\n",
        "Extract embeddings specifically for the word \"fly\" from both sentences."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "y8ZvAkGiJLZf"
      },
      "outputs": [],
      "source": [
        "# Extracting embeddings for the word 'fly' in both sentences\n",
        "emb1 = out1[0:, tokens1.index(\"fly\"), :].detach()[0]\n",
        "emb2 = out2[0:, tokens2.index(\"fly\"), :].detach()[0]\n",
        "\n",
        "# emb1 = out1[0:, 3, :].detach()\n",
        "# emb2 = out2[0:, 3, :].detach()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gcvcIGiOpLyr"
      },
      "source": [
        "## üìä Calculating Cosine Similarity\n",
        "\n",
        "Calculate the cosine similarity between the embeddings of the word \"fly\" from both sentences to measure how context affects meaning."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GKVSZUxyJNZs",
        "outputId": "b389c0d3-3650-45a2-9b4a-8b425f8a57ef"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "0.06798799469885963"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Calculating the cosine similarity between the embeddings\n",
        "cosine(emb1, emb2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gfFR3mbspN8y"
      },
      "source": [
        "## üåü Conclusion\n",
        "\n",
        "This notebook has guided you through the process of extracting and comparing word embeddings using BERT. Such techniques are fundamental in understanding word semantics and their usage across different contexts.\n",
        "\n",
        "Experiment by changing the sentences or focusing on different words to see how the embeddings and their similarities vary!"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.19"
    },
    "orig_nbformat": 4
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
